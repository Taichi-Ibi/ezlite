{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ezlite import *\n",
    "import modules as el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Taichi-Ibi/ezlite\n",
      "  Cloning https://github.com/Taichi-Ibi/ezlite to /private/var/folders/83/_g6bdl9112v0jvxbvhjfsyk00000gn/T/pip-req-build-571rebox\n",
      "  Running command git clone -q https://github.com/Taichi-Ibi/ezlite /private/var/folders/83/_g6bdl9112v0jvxbvhjfsyk00000gn/T/pip-req-build-571rebox\n",
      "  Resolved https://github.com/Taichi-Ibi/ezlite to commit 4a124110f823ca49763e0838f23b02e88621afc9\n",
      "Building wheels for collected packages: ezlite\n",
      "  Building wheel for ezlite (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ezlite: filename=ezlite-0.198-py3-none-any.whl size=5519 sha256=cba43ebfbfa3fd1ea13576c65cff0f47019fab50e29f7c59a2799bd0421d4d1d\n",
      "  Stored in directory: /private/var/folders/83/_g6bdl9112v0jvxbvhjfsyk00000gn/T/pip-ephem-wheel-cache-jykyrj2u/wheels/88/e3/52/b184c1259cee1c2f49640453a353920f44010ad95cd0877837\n",
      "Successfully built ezlite\n",
      "Installing collected packages: ezlite\n",
      "  Attempting uninstall: ezlite\n",
      "    Found existing installation: ezlite 0.197\n",
      "    Uninstalling ezlite-0.197:\n",
      "      Successfully uninstalled ezlite-0.197\n",
      "Successfully installed ezlite-0.198\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/Taichi-Ibi/ezlite --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ヒット数が20を超えたので検索を中断しました。\n",
      "- /Users/estyle-085/Workspace/_past/tdualKenshu/M2Det(Archived)/code/train.ipynb 1\n",
      "  22  from torch.autograd import Variable\n",
      "  23  \n",
      "* 24  import torchvision.transforms as transforms\n",
      "  25  import torchvision.models as models\n",
      "  26  \n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/_past/tdualKenshu/共有/train.ipynb 3\n",
      "     9  \n",
      "    10  import imageio\n",
      "*   11  import skimage.transform\n",
      "    12  from tqdm.notebook import tqdm\n",
      "    13  \n",
      "\n",
      "\n",
      "   637      for img, box in batch:\n",
      "   638          img = np.array(img)\n",
      "*  639          img = skimage.transform.resize(img, [608, 608, 3], mode=\"reflect\")\n",
      "   640          images.append([img])\n",
      "   641          bboxes.append([box])\n",
      "\n",
      "\n",
      "  1093  \n",
      "  1094      for images, targets in data_loader:\n",
      "* 1095          model_input = [[skimage.transform.resize(img, [width, height, 3])] for img in images]\n",
      "  1096          model_input = np.concatenate(model_input, axis=0)\n",
      "  1097          model_input = model_input.transpose(0, 3, 1, 2)\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/Training/KaggleTitanic/matome.ipynb 1\n",
      "  45  - copy() 変数のコピーにアクセス\n",
      "  46  - drop() データの削除\n",
      "* 47  - LabelEncoder() 文字列ごとに数値化。fitとtransformを使う\n",
      "  48  - cut() binごとにデータを区分け\n",
      "  49  - query() 条件でdfを指定\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/Training/KaggleTitanic/main/titanic_tableau.ipynb 4\n",
      "  49  # le = LabelEncoder()\n",
      "  50  # le.fit(df['Sex'])\n",
      "* 51  # df['Sex'] = le.transform(df['Sex'])\n",
      "  52  # le.fit(df['Embarked'])\n",
      "* 53  # df['Embarked'] = le.transform(df['Embarked'])\n",
      "  54  # le.fit(df['Cabin'])\n",
      "* 55  # df['Cabin'] = le.transform(df['Cabin'])\n",
      "  56  # # df['Cabin'] = [1 if x>1 else 0 for x in df['Cabin']] #Cabin 1以上の場合は全て1\n",
      "  57  # le.fit(df['Title'])\n",
      "* 58  # df['Title'] = le.transform(df['Title'])\n",
      "  59  \n",
      "  60  # def title_to_num(title):\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/Training/KaggleTitanic/main/titanic.ipynb 6\n",
      "   88  le = LabelEncoder()\n",
      "   89  le.fit(df['Sex'])\n",
      "*  90  df['Sex'] = le.transform(df['Sex'])\n",
      "   91  le.fit(df['Embarked'])\n",
      "*  92  df['Embarked'] = le.transform(df['Embarked'])\n",
      "   93  le.fit(df['Cabin_init'])\n",
      "*  94  df['Cabin_init'] = le.transform(df['Cabin_init'])\n",
      "   95  le.fit(df['Fare_bin'])\n",
      "*  96  df['Fare_bin'] = le.transform(df['Fare_bin'])\n",
      "   97  le.fit(df['Age_bin'])\n",
      "*  98  df['Age_bin'] = le.transform(df['Age_bin'])\n",
      "   99  le.fit(df['Title'])\n",
      "* 100  df['Title'] = le.transform(df['Title'])\n",
      "  101  df.isnull().sum()\n",
      "  102  df.drop([\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/Training/Python100Knoks/Part4/4章_顧客の行動を予測する１０本ノック_answer.ipynb 2\n",
      "  19  from sklearn.preprocessing import StandardScaler\n",
      "  20  sc = StandardScaler()\n",
      "* 21  customer_clustering_sc = sc.fit_transform(customer_clustering)\n",
      "  22  \n",
      "  23  kmeans = KMeans(n_clusters=4, random_state=0)\n",
      "\n",
      "\n",
      "  35  pca = PCA(n_components=2)\n",
      "  36  pca.fit(X)\n",
      "* 37  x_pca = pca.transform(X)\n",
      "  38  pca_df = pd.DataFrame(x_pca)\n",
      "  39  pca_df[\"cluster\"] = customer_clustering[\"cluster\"]\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/Training/Python100Knoks/Part4/Part4-1st.ipynb 2\n",
      "  16  from sklearn.preprocessing import StandardScaler\n",
      "  17  sc = StandardScaler()\n",
      "* 18  customer_clustering_sc = sc.fit_transform(customer_clustering)\n",
      "  19  \n",
      "  20  kmeans = KMeans(n_clusters=4, random_state=0)\n",
      "\n",
      "\n",
      "  32  pca = PCA(n_components=2)\n",
      "  33  pca.fit(X)\n",
      "* 34  x_pca = pca.transform(X)\n",
      "  35  pca_df = pd.DataFrame(x_pca)\n",
      "  36  pca_df['cluster'] = customer_clustering['cluster']\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/Training/Python100Knoks/Part4/Part4-2nd.ipynb 2\n",
      "  16  from sklearn.preprocessing import StandardScaler\n",
      "  17  sc = StandardScaler()\n",
      "* 18  customer_clustering_sc = sc.fit_transform(customer_clustering)\n",
      "  19  \n",
      "  20  kmeans = KMeans(n_clusters=4, random_state=0)\n",
      "\n",
      "\n",
      "  32  pca = PCA(n_components=2)\n",
      "  33  pca.fit(X)\n",
      "* 34  x_pca = pca.transform(X)1\n",
      "  35  pca_df = pd.DataFrame(x_pca)\n",
      "  36  pca_df['cluster'] = customer_clustering['cluster']\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/Training/KaggleBikeSharingDemand/main/model_elnet.ipynb 3\n",
      "  23  print(clf.intercept_)\n",
      "  24  print(mse)\n",
      "* 25  y_pred = clf.predict(scaler.transform(X_test))\n",
      "  26  # #Lasso回帰\n",
      "  27  # from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "  33  \n",
      "  34  # scaler.fit(X_train)\n",
      "* 35  # clf.fit(scaler.transform(X_train), y_train)\n",
      "  36  \n",
      "* 37  # y_pred = clf.predict(scaler.transform(X_test))\n",
      "  38  # mse = mean_squared_error(y_test, y_pred)\n",
      "  39  \n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/Training/KaggleBikeSharingDemand/main/model_elnet-Copy1.ipynb 3\n",
      "  23  print(clf.intercept_)\n",
      "  24  print(mse)\n",
      "* 25  y_pred = clf.predict(scaler.transform(X_test))\n",
      "  26  # #Lasso回帰\n",
      "  27  # from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "  33  \n",
      "  34  # scaler.fit(X_train)\n",
      "* 35  # clf.fit(scaler.transform(X_train), y_train)\n",
      "  36  \n",
      "* 37  # y_pred = clf.predict(scaler.transform(X_test))\n",
      "  38  # mse = mean_squared_error(y_test, y_pred)\n",
      "  39  \n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/Training/KaggleBikeSharingDemand/main/viz.ipynb 2\n",
      "  52  # le = LabelEncoder()\n",
      "  53  # le.fit(train['weekday'])\n",
      "* 54  # train['weekday'] = le.transform(train['weekday'])\n",
      "  55  # le.fit(train['day_type'])\n",
      "* 56  # train['day_type'] = le.transform(train['day_type'])\n",
      "  57  # train = pd.get_dummies(train, columns=['weather', 'weekday', 'season_rev', 'day_type'])\n",
      "  58  train.head()\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/Training/KaggleBikeSharingDemand/main/toTableau.ipynb 9\n",
      "   77  le = LabelEncoder()\n",
      "   78  le.fit(df['weekday'])\n",
      "*  79  df['weekday'] = le.transform(df['weekday'])\n",
      "   80  le.fit(df['day_type'])\n",
      "*  81  df['day_type'] = le.transform(df['day_type'])\n",
      "   82  # casualとregisteredを対数変換\n",
      "   83  # 0を含むため全データに1を足してから対数変換\n",
      "\n",
      "\n",
      "   88  # display(df.loc[df['casual'].values > 0, 'casual'].min())\n",
      "   89  \n",
      "*  90  # def log_transform(trg):\n",
      "   91  #     df[trg] = df[trg]/(df[trg].max()) #0~1になる\n",
      "   92  #     count_min = df.loc[df['casual'].values > 0, 'casual'].min()\n",
      "\n",
      "\n",
      "   95      \n",
      "   96  \n",
      "*  97  # log_transform('casual')\n",
      "*  98  # log_transform('registered')\n",
      "   99  df.head()\n",
      "  100  df.describe()\n",
      "\n",
      "\n",
      "  143  df.to_csv('output/df_prepro.csv',index=False)\n",
      "  144  # df = pd.read_csv('output/df_prepro.csv')\n",
      "* 145  # def y_transform(df, col):\n",
      "  146  #     df[col] = np.sqrt(df[col])\n",
      "  147  #     # count_min  = df.loc[df[col].values > 0, col].min() #0を除いた最小値\n",
      "\n",
      "\n",
      "  149  #     # df[col] = np.log(df[col])*10\n",
      "  150      \n",
      "* 151  # y_transform(train, 'casual')\n",
      "* 152  # y_transform(train, 'registered')\n",
      "* 153  # y_transform(train, 'count')\n",
      "  154  \n",
      "  155  # fig=plt.figure(figsize=(20,5))\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/ESTYLE-U/2022-10/sub0/ibi_taichi_1_3.ipynb 9\n",
      "  180  num_cols\n",
      "  181  # 前処理\n",
      "* 182  numeric_transformer = Pipeline(\n",
      "  183      steps=[(\"scaler\", StandardScaler())]\n",
      "  184  )\n",
      "  185  \n",
      "* 186  categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
      "  187  \n",
      "  188  preprocessor = ColumnTransformer(\n",
      "* 189      transformers=[\n",
      "* 190          (\"num\", numeric_transformer, num_cols),\n",
      "* 191          (\"cat\", categorical_transformer, cat_cols),\n",
      "  192      ]\n",
      "  193  )\n",
      "\n",
      "\n",
      "  204    tr_y, val_y, pred_base = df[is_tr]['num_rides'], df[is_val]['num_rides'], df[is_val]['num_rides_lag7']\n",
      "  205  \n",
      "* 206    tr_x = preprocessor.fit_transform(tr_x)\n",
      "* 207    val_x = preprocessor.transform(val_x)\n",
      "  208  \n",
      "  209    model.fit(\n",
      "\n",
      "\n",
      "  249  tr_y, val_y, pred_base = df1[is_tr]['num_rides'], df1[is_val]['num_rides'], df1[is_val]['num_rides_lag7']\n",
      "  250  \n",
      "* 251  tr_x = preprocessor.fit_transform(tr_x)\n",
      "* 252  val_x = preprocessor.transform(val_x)\n",
      "  253  ### MAE\n",
      "  254  rg = ElasticNet(alpha = study.best_params['alpha'], l1_ratio=study.best_params['l1_ratio'])\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/ESTYLE-U/2022-10/sub0/ibi_taichi_1_1.ipynb 9\n",
      "  188  num_cols\n",
      "  189  # 前処理\n",
      "* 190  numeric_transformer = Pipeline(\n",
      "  191      steps=[(\"scaler\", StandardScaler())]\n",
      "  192  )\n",
      "  193  \n",
      "* 194  categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
      "  195  \n",
      "  196  preprocessor = ColumnTransformer(\n",
      "* 197      transformers=[\n",
      "* 198          (\"num\", numeric_transformer, num_cols),\n",
      "* 199          (\"cat\", categorical_transformer, cat_cols),\n",
      "  200      ]\n",
      "  201  )\n",
      "\n",
      "\n",
      "  213    tr_y, val_y = df[is_tr]['num_rides'], df[is_val]['num_rides']\n",
      "  214  \n",
      "* 215    tr_x = preprocessor.fit_transform(tr_x)\n",
      "* 216    val_x = preprocessor.transform(val_x)\n",
      "  217  \n",
      "  218    model.fit(\n",
      "\n",
      "\n",
      "  258  tr_y, val_y = df1[is_tr]['num_rides'], df1[is_val]['num_rides']\n",
      "  259  \n",
      "* 260  tr_x = preprocessor.fit_transform(tr_x)\n",
      "* 261  val_x = preprocessor.transform(val_x)\n",
      "  262  ### MAE\n",
      "  263  rg = ElasticNet(alpha = study.best_params['alpha'], l1_ratio=study.best_params['l1_ratio'])\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/ESTYLE-U/2022-10/sub0/ibi_taichi_1_2.ipynb 9\n",
      "  187  num_cols\n",
      "  188  # 前処理\n",
      "* 189  numeric_transformer = Pipeline(\n",
      "  190      steps=[(\"scaler\", StandardScaler())]\n",
      "  191  )\n",
      "  192  \n",
      "* 193  categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
      "  194  \n",
      "  195  preprocessor = ColumnTransformer(\n",
      "* 196      transformers=[\n",
      "* 197          (\"num\", numeric_transformer, num_cols),\n",
      "* 198          (\"cat\", categorical_transformer, cat_cols),\n",
      "  199      ]\n",
      "  200  )\n",
      "\n",
      "\n",
      "  212    tr_y, val_y = df[is_tr]['num_rides'], df[is_val]['num_rides']\n",
      "  213  \n",
      "* 214    tr_x = preprocessor.fit_transform(tr_x)\n",
      "* 215    val_x = preprocessor.transform(val_x)\n",
      "  216  \n",
      "  217    model.fit(\n",
      "\n",
      "\n",
      "  257  tr_y, val_y = df1[is_tr]['num_rides'], df1[is_val]['num_rides']\n",
      "  258  \n",
      "* 259  tr_x = preprocessor.fit_transform(tr_x)\n",
      "* 260  val_x = preprocessor.transform(val_x)\n",
      "  261  ### MAE\n",
      "  262  rg = ElasticNet(alpha = study.best_params['alpha'], l1_ratio=study.best_params['l1_ratio'])\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/ESTYLE-U/2022-10/submission1/ibi_taichi_2_prev.ipynb 1\n",
      "  199  \n",
      "  200  lbl = LabelEncoder()\n",
      "* 201  df_all[cat] = df_all[cat].apply(lbl.fit_transform)\n",
      "  202  df_all.head()\n",
      "  203  # ラグ変数にnaがある期間を削除\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/ESTYLE-U/2022-10/submission1/ibi_taichi_2_next.ipynb 1\n",
      "  198  \n",
      "  199  lbl = LabelEncoder()\n",
      "* 200  df_all[cat] = df_all[cat].apply(lbl.fit_transform)\n",
      "  201  df_all.head()\n",
      "  202  # ラグ変数にnaがある期間を削除\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/ESTYLE-U/2022-10/submission1/ibi_taichi_1_3.ipynb 9\n",
      "  179  num_cols\n",
      "  180  # 前処理\n",
      "* 181  numeric_transformer = Pipeline(\n",
      "  182      steps=[(\"scaler\", StandardScaler())]\n",
      "  183  )\n",
      "  184  \n",
      "* 185  categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
      "  186  \n",
      "  187  preprocessor = ColumnTransformer(\n",
      "* 188      transformers=[\n",
      "* 189          (\"num\", numeric_transformer, num_cols),\n",
      "* 190          (\"cat\", categorical_transformer, cat_cols),\n",
      "  191      ]\n",
      "  192  )\n",
      "\n",
      "\n",
      "  203    tr_y, val_y, pred_base = df[is_tr]['num_rides'], df[is_val]['num_rides'], df[is_val]['num_rides_lag7']\n",
      "  204  \n",
      "* 205    tr_x = preprocessor.fit_transform(tr_x)\n",
      "* 206    val_x = preprocessor.transform(val_x)\n",
      "  207  \n",
      "  208    model.fit(\n",
      "\n",
      "\n",
      "  247  tr_y, val_y, pred_base = df1[is_tr]['num_rides'], df1[is_val]['num_rides'], df1[is_val]['num_rides_lag7']\n",
      "  248  \n",
      "* 249  tr_x = preprocessor.fit_transform(tr_x)\n",
      "* 250  val_x = preprocessor.transform(val_x)\n",
      "  251  # MAE\n",
      "  252  \n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/ESTYLE-U/2022-10/submission1/ibi_taichi_2_0.ipynb 1\n",
      "  170  \n",
      "  171  lbl = LabelEncoder()\n",
      "* 172  df_all[cat] = df_all[cat].apply(lbl.fit_transform)\n",
      "  173  df_all.head()\n",
      "  174  # ラグ変数にnaがある期間を削除\n",
      "\n",
      "\n",
      "- /Users/estyle-085/Workspace/ESTYLE-U/2022-10/submission1/ibi_taichi_1_1.ipynb 9\n",
      "  186  num_cols\n",
      "  187  # 前処理\n",
      "* 188  numeric_transformer = Pipeline(\n",
      "  189      steps=[(\"scaler\", StandardScaler())]\n",
      "  190  )\n",
      "  191  \n",
      "* 192  categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
      "  193  \n",
      "  194  preprocessor = ColumnTransformer(\n",
      "* 195      transformers=[\n",
      "* 196          (\"num\", numeric_transformer, num_cols),\n",
      "* 197          (\"cat\", categorical_transformer, cat_cols),\n",
      "  198      ]\n",
      "  199  )\n",
      "\n",
      "\n",
      "  211    tr_y, val_y = df[is_tr]['num_rides'], df[is_val]['num_rides']\n",
      "  212  \n",
      "* 213    tr_x = preprocessor.fit_transform(tr_x)\n",
      "* 214    val_x = preprocessor.transform(val_x)\n",
      "  215  \n",
      "  216    model.fit(\n",
      "\n",
      "\n",
      "  256  tr_y, val_y = df1[is_tr]['num_rides'], df1[is_val]['num_rides']\n",
      "  257  \n",
      "* 258  tr_x = preprocessor.fit_transform(tr_x)\n",
      "* 259  val_x = preprocessor.transform(val_x)\n",
      "  260  # MAE\n",
      "  261  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "el.sniff('transform', 'Workspace/**/*.ipynb')\n",
    "# sniff('transform', '/Users/estyle-085/WorkSpace/_past/tdualKenshu/M2Det(Archived)/code/M2Det/test.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kenshu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb050f06ecfa28ed6828977e08233fb082b8fdd21b38d96a76c0d878db2d90af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
